{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benspilsbury/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benspilsbury/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/benspilsbury/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Removal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',\n",
    "            'said', 'one', 'com', 'satirewire', '-', '–', '—', 'satirewire.com']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "def process_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV and Split into Apple and Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding = 'unicode_escape')\n",
    "tweets.tweet_text=tweets.tweet_text.map(lambda x: str(x))\n",
    "tweets = tweets[tweets.is_there_an_emotion_directed_at_a_brand_or_product != \"I can't tell\"]\n",
    "tweets=tweets.replace('Negative emotion', 0)\n",
    "tweets=tweets.replace('Positive emotion', 1)\n",
    "tweets=tweets.replace('No emotion toward brand or product', 2)\n",
    "tweets.is_there_an_emotion_directed_at_a_brand_or_product=pd.to_numeric(tweets.is_there_an_emotion_directed_at_a_brand_or_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple=tweets[(tweets.emotion_in_tweet_is_directed_at=='iPad') | \n",
    "      (tweets.emotion_in_tweet_is_directed_at=='Apple') |\n",
    "      (tweets.emotion_in_tweet_is_directed_at=='iPad or iPhone App') | \n",
    "      (tweets.emotion_in_tweet_is_directed_at=='iPhone') |\n",
    "      (tweets.emotion_in_tweet_is_directed_at=='Other Apple product or service')]\n",
    "google=tweets[(tweets.emotion_in_tweet_is_directed_at=='Google') | \n",
    "      (tweets.emotion_in_tweet_is_directed_at=='Other Google product or service') |\n",
    "      (tweets.emotion_in_tweet_is_directed_at=='Android App') | \n",
    "      (tweets.emotion_in_tweet_is_directed_at=='Android')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.tweet_text=apple['tweet_text'].map(lambda x: lemmatizer.lemmatize(x))\n",
    "google.tweet_text=google['tweet_text'].map(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv_apple = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "cv_google = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "apple_text_counts= cv_apple.fit_transform(apple['tweet_text'])\n",
    "google_text_counts= cv_google.fit_transform(google['tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_df = pd.DataFrame(apple_text_counts.todense(), columns = cv_apple.get_feature_names())\n",
    "google_df = pd.DataFrame(google_text_counts.todense(),columns = cv_google.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to Remove SXSW and Mention Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_df=apple_df.drop(['sxsw', 'mention'], axis=1)\n",
    "google_df=google_df.drop(['sxsw', 'mention'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish features and target in data\n",
    "\n",
    "gX=google_df\n",
    "gy=google['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "# Split features and target into training and testing partitions\n",
    "\n",
    "gX_train, gX_test, gy_train, gy_test = train_test_split(gX, gy, random_state=1)\n",
    "\n",
    "# Scale feature set according to standard distribution to make results more interpretable\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(gX_train)\n",
    "gX_train = scaler.transform(gX_train)  \n",
    "gX_test = scaler.transform(gX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "0.7350707634573063\n"
     ]
    }
   ],
   "source": [
    "# Establish decision tree model\n",
    "\n",
    "gdtc = DecisionTreeClassifier(criterion= 'gini', class_weight='balanced', random_state=1)\n",
    "\n",
    "# Fit decision tree model to training data\n",
    "\n",
    "gdtc.fit(gX_train, gy_train)\n",
    "\n",
    "# Make predictions on test set with model\n",
    "\n",
    "gdtc_preds  = gdtc.predict(gX_test)\n",
    "\n",
    "print(accuracy_score(gy_test, gdtc_preds))\n",
    "print(f1_score(gy_test, gdtc_preds, pos_label='positive', average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish features and target in data\n",
    "\n",
    "aX=apple_df\n",
    "ay=apple['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "# Split features and target into training and testing partitions\n",
    "\n",
    "aX_train, aX_test, ay_train, ay_test = train_test_split(aX, ay, random_state=1)\n",
    "\n",
    "# Scale feature set according to standard distribution to make results more interpretable\n",
    "ascaler = StandardScaler()\n",
    "ascaler.fit(aX_train)\n",
    "aX_train = ascaler.transform(aX_train)  \n",
    "aX_test = ascaler.transform(aX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304492512479202\n",
      "0.744511417932411\n"
     ]
    }
   ],
   "source": [
    "# Establish decision tree model\n",
    "\n",
    "adtc = DecisionTreeClassifier(criterion= 'gini', class_weight='balanced', random_state=1)\n",
    "\n",
    "# Fit decision tree model to training data\n",
    "\n",
    "adtc.fit(aX_train, ay_train)\n",
    "\n",
    "# Make predictions on test set with model\n",
    "\n",
    "adtc_preds  = adtc.predict(aX_test)\n",
    "\n",
    "print(accuracy_score(ay_test, adtc_preds))\n",
    "print(f1_score(ay_test, adtc_preds, pos_label='positive', average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "afreqpos=apple[apple['is_there_an_emotion_directed_at_a_brand_or_product']==1]\n",
    "afreqneg=apple[apple['is_there_an_emotion_directed_at_a_brand_or_product']==0]\n",
    "afreqneut=apple[apple['is_there_an_emotion_directed_at_a_brand_or_product']==2]\n",
    "\n",
    "adatapos=afreqpos.tweet_text\n",
    "adataneg=afreqneg.tweet_text\n",
    "adataneut=afreqneut.tweet_text\n",
    "\n",
    "apos = list(map(process_article, adatapos))\n",
    "aneg = list(map(process_article, adataneg))\n",
    "aneut = list(map(process_article, adataneut))\n",
    "\n",
    "atotal_vocab_pos = set()\n",
    "for comment in apos:\n",
    "    atotal_vocab_pos.update(comment)\n",
    "\n",
    "atotal_vocab_neg = set()\n",
    "for comment in aneg:\n",
    "    atotal_vocab_neg.update(comment)\n",
    "\n",
    "atotal_vocab_neut = set()\n",
    "for comment in aneut:\n",
    "    atotal_vocab_neut.update(comment)\n",
    "    \n",
    "flat_apos = [item for sublist in apos for item in sublist]\n",
    "flat_aneg = [item for sublist in aneg for item in sublist]\n",
    "flat_aneut = [item for sublist in aneut for item in sublist]\n",
    "\n",
    "apos_freq = FreqDist(flat_apos)\n",
    "aneg_freq = FreqDist(flat_aneg)\n",
    "aneut_freq = FreqDist(flat_aneut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.08416\n",
      "mention \t\t 0.0586\n",
      "ipad \t\t 0.03733\n",
      "apple \t\t 0.0346\n",
      "link \t\t 0.03415\n",
      "rt \t\t 0.02453\n",
      "store \t\t 0.02013\n",
      "iphone \t\t 0.0185\n",
      "2 \t\t 0.01752\n",
      "'s \t\t 0.01365\n",
      "app \t\t 0.01198\n",
      "quot \t\t 0.01019\n",
      "austin \t\t 0.009699\n",
      "new \t\t 0.008028\n",
      "ipad2 \t\t 0.00758\n",
      "pop-up \t\t 0.005705\n",
      "amp \t\t 0.005013\n",
      "n't \t\t 0.004768\n",
      "line \t\t 0.004646\n",
      "get \t\t 0.004524\n",
      "cool \t\t 0.00379\n",
      "'m \t\t 0.003709\n",
      "via \t\t 0.003383\n",
      "opening \t\t 0.003383\n",
      "temporary \t\t 0.003383\n"
     ]
    }
   ],
   "source": [
    "apos_freq.most_common(20)\n",
    "\n",
    "apos_total_word_count = sum(apos_freq.values())\n",
    "apos_top_25 = apos_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in apos_top_25:\n",
    "    normalized_frequency = word[1]/apos_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.08209\n",
      "mention \t\t 0.03951\n",
      "ipad \t\t 0.03603\n",
      "iphone \t\t 0.03009\n",
      "quot \t\t 0.02354\n",
      "apple \t\t 0.02313\n",
      "rt \t\t 0.01617\n",
      "n't \t\t 0.0131\n",
      "'s \t\t 0.01228\n",
      "link \t\t 0.01187\n",
      "2 \t\t 0.01167\n",
      "app \t\t 0.009621\n",
      "store \t\t 0.008598\n",
      "like \t\t 0.006346\n",
      "design \t\t 0.004913\n",
      "apps \t\t 0.004913\n",
      "people \t\t 0.004913\n",
      "austin \t\t 0.004708\n",
      "new \t\t 0.004504\n",
      "amp \t\t 0.004094\n",
      "would \t\t 0.003889\n",
      "get \t\t 0.00348\n",
      "need \t\t 0.003275\n",
      "news \t\t 0.003275\n",
      "battery \t\t 0.003071\n"
     ]
    }
   ],
   "source": [
    "aneg_freq.most_common(20)\n",
    "\n",
    "aneg_total_word_count = sum(aneg_freq.values())\n",
    "aneg_top_25 = aneg_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in aneg_top_25:\n",
    "    normalized_frequency = word[1]/aneg_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.08649\n",
      "mention \t\t 0.04621\n",
      "ipad \t\t 0.03555\n",
      "apple \t\t 0.03436\n",
      "link \t\t 0.02725\n",
      "store \t\t 0.02133\n",
      "iphone \t\t 0.02014\n",
      "rt \t\t 0.01896\n",
      "quot \t\t 0.0154\n",
      "n't \t\t 0.01185\n",
      "'s \t\t 0.01066\n",
      "pop-up \t\t 0.009479\n",
      "austin \t\t 0.009479\n",
      "2 \t\t 0.009479\n",
      "app \t\t 0.008294\n",
      "new \t\t 0.008294\n",
      "'m \t\t 0.007109\n",
      "call \t\t 0.005924\n",
      "line \t\t 0.005924\n",
      "still \t\t 0.004739\n",
      "ipad2 \t\t 0.004739\n",
      "design \t\t 0.004739\n",
      "get \t\t 0.004739\n",
      "'re \t\t 0.003555\n",
      "front \t\t 0.003555\n"
     ]
    }
   ],
   "source": [
    "aneut_freq.most_common(20)\n",
    "\n",
    "aneut_total_word_count = sum(aneut_freq.values())\n",
    "aneut_top_25 = aneut_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in aneut_top_25:\n",
    "    normalized_frequency = word[1]/aneut_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfreqpos=google[google['is_there_an_emotion_directed_at_a_brand_or_product']==1]\n",
    "gfreqneg=google[google['is_there_an_emotion_directed_at_a_brand_or_product']==0]\n",
    "gfreqneut=google[google['is_there_an_emotion_directed_at_a_brand_or_product']==2]\n",
    "\n",
    "gdatapos=gfreqpos.tweet_text\n",
    "gdataneg=gfreqneg.tweet_text\n",
    "gdataneut=gfreqneut.tweet_text\n",
    "\n",
    "gpos = list(map(process_article, gdatapos))\n",
    "gneg = list(map(process_article, gdataneg))\n",
    "gneut = list(map(process_article, gdataneut))\n",
    "\n",
    "gtotal_vocab_pos = set()\n",
    "for comment in gpos:\n",
    "    gtotal_vocab_pos.update(comment)\n",
    "\n",
    "gtotal_vocab_neg = set()\n",
    "for comment in gneg:\n",
    "    gtotal_vocab_neg.update(comment)\n",
    "\n",
    "gtotal_vocab_neut = set()\n",
    "for comment in gneut:\n",
    "    gtotal_vocab_neut.update(comment)\n",
    "    \n",
    "flat_gpos = [item for sublist in gpos for item in sublist]\n",
    "flat_gneg = [item for sublist in gneg for item in sublist]\n",
    "flat_gneut = [item for sublist in gneut for item in sublist]\n",
    "\n",
    "gpos_freq = FreqDist(flat_gpos)\n",
    "gneg_freq = FreqDist(flat_gneg)\n",
    "gneut_freq = FreqDist(flat_gneut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.07547\n",
      "google \t\t 0.06409\n",
      "mention \t\t 0.05251\n",
      "link \t\t 0.02599\n",
      "rt \t\t 0.02401\n",
      "quot \t\t 0.01691\n",
      "android \t\t 0.01503\n",
      "new \t\t 0.01263\n",
      "'s \t\t 0.01117\n",
      "circles \t\t 0.009812\n",
      "maps \t\t 0.009603\n",
      "party \t\t 0.009395\n",
      "social \t\t 0.008873\n",
      "network \t\t 0.007411\n",
      "launch \t\t 0.007098\n",
      "app \t\t 0.006785\n",
      "mobile \t\t 0.006785\n",
      "amp \t\t 0.006367\n",
      "mayer \t\t 0.005846\n",
      "great \t\t 0.005428\n",
      "marissa \t\t 0.005324\n",
      "today \t\t 0.005324\n",
      "called \t\t 0.005219\n",
      "major \t\t 0.004802\n",
      "time \t\t 0.004697\n"
     ]
    }
   ],
   "source": [
    "gpos_freq.most_common(20)\n",
    "\n",
    "gpos_total_word_count = sum(gpos_freq.values())\n",
    "gpos_top_25 = gpos_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in gpos_top_25:\n",
    "    normalized_frequency = word[1]/gpos_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.07603\n",
      "google \t\t 0.07197\n",
      "mention \t\t 0.05281\n",
      "rt \t\t 0.0267\n",
      "quot \t\t 0.02554\n",
      "link \t\t 0.01683\n",
      "circles \t\t 0.01393\n",
      "social \t\t 0.01335\n",
      "new \t\t 0.009867\n",
      "android \t\t 0.009286\n",
      "launch \t\t 0.008706\n",
      "n't \t\t 0.007545\n",
      "today \t\t 0.006965\n",
      "'s \t\t 0.006384\n",
      "network \t\t 0.005804\n",
      "bing \t\t 0.005223\n",
      "major \t\t 0.005223\n",
      "mayer \t\t 0.005223\n",
      "maps \t\t 0.005223\n",
      "product \t\t 0.004643\n",
      "much \t\t 0.004643\n",
      "called \t\t 0.004063\n",
      "needs \t\t 0.004063\n",
      "app \t\t 0.004063\n",
      "tv \t\t 0.004063\n"
     ]
    }
   ],
   "source": [
    "gneg_freq.most_common(20)\n",
    "\n",
    "gneg_total_word_count = sum(gneg_freq.values())\n",
    "gneg_top_25 = gneg_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in gneg_top_25:\n",
    "    normalized_frequency = word[1]/gneg_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \t\t Normalized Frequency\n",
      "\n",
      "sxsw \t\t 0.08571\n",
      "google \t\t 0.07937\n",
      "mention \t\t 0.04127\n",
      "link \t\t 0.0381\n",
      "quot \t\t 0.03175\n",
      "'s \t\t 0.02857\n",
      "circles \t\t 0.0254\n",
      "social \t\t 0.02222\n",
      "launch \t\t 0.01587\n",
      "network \t\t 0.01587\n",
      "rt \t\t 0.01587\n",
      "new \t\t 0.0127\n",
      "party \t\t 0.009524\n",
      "like \t\t 0.009524\n",
      "n't \t\t 0.009524\n",
      "diller \t\t 0.006349\n",
      "product \t\t 0.006349\n",
      "called \t\t 0.006349\n",
      "today \t\t 0.006349\n",
      "android \t\t 0.006349\n",
      "socialmedia \t\t 0.006349\n",
      "w/ \t\t 0.006349\n",
      "teams \t\t 0.006349\n",
      "making \t\t 0.006349\n",
      "actual \t\t 0.006349\n"
     ]
    }
   ],
   "source": [
    "gneut_freq.most_common(20)\n",
    "\n",
    "gneut_total_word_count = sum(gneut_freq.values())\n",
    "gneut_top_25 = gneut_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in gneut_top_25:\n",
    "    normalized_frequency = word[1]/gneut_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf=TfidfVectorizer()\n",
    "gtext_tf= gtf.fit_transform(google['tweet_text'])\n",
    "\n",
    "atf=TfidfVectorizer()\n",
    "atext_tf= atf.fit_transform(apple['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtdif=pd.DataFrame(gtext_tf.toarray().transpose(), index = gtf.get_feature_names()).transpose()\n",
    "atdif=pd.DataFrame(atext_tf.toarray().transpose(), index = atf.get_feature_names()).transpose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100s</th>\n",
       "      <th>100tc</th>\n",
       "      <th>101</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>ûïbuttons</th>\n",
       "      <th>ûïfoursquare</th>\n",
       "      <th>ûïline</th>\n",
       "      <th>ûïmute</th>\n",
       "      <th>ûïthe</th>\n",
       "      <th>ûïview</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûó</th>\n",
       "      <th>ûójust</th>\n",
       "      <th>ûólewis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2402 rows × 4573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000   02   03   08   10  100  100s  100tc  101  106  ...  ûïbuttons  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "...   ...  ...  ...  ...  ...  ...   ...    ...  ...  ...  ...        ...   \n",
       "2397  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "2398  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "2399  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "2400  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "2401  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  ...        0.0   \n",
       "\n",
       "      ûïfoursquare  ûïline  ûïmute  ûïthe  ûïview   ûò   ûó  ûójust  ûólewis  \n",
       "0              0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "1              0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "2              0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "3              0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "4              0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "...            ...     ...     ...    ...     ...  ...  ...     ...      ...  \n",
       "2397           0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "2398           0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "2399           0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "2400           0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "2401           0.0     0.0     0.0    0.0     0.0  0.0  0.0     0.0      0.0  \n",
       "\n",
       "[2402 rows x 4573 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=['wesley83 3g iphone 3 hr tweeting rise austin dead need upgrade plugin station sxsw',\n",
    " 'know fludapp awesome ipad iphone app likely appreciate design also giving free t sxsw',\n",
    " 'wait ipad 2 also sale sxsw',\n",
    " 'hope year festival crashy year iphone app sxsw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wesley83',\n",
       " '3g',\n",
       " 'iphone',\n",
       " '3',\n",
       " 'hr',\n",
       " 'tweeting',\n",
       " 'rise',\n",
       " 'austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'station',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1949\n",
      "388\n",
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8114071606994172"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(afreqpos))\n",
    "print(len(afreqneg))\n",
    "print(len(afreqneut))\n",
    "\n",
    "1949/(1949+388+65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))   \n",
    "mat = confusion_matrix(ay_test, adtc_preds)\n",
    "ax = sns.heatmap(mat, cbar=False, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
